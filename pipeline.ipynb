{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ivang/miniconda3/envs/TagGPT/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.representation import PartOfSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_model = SentenceTransformer(\"Salesforce/SFR-Embedding-2_R\")\n",
    "# embedding_model = SentenceTransformer(\"mixedbread-ai/mxbai-embed-large-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('twitter_dataset.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "data = fetch_20newsgroups(subset='all', remove=('headers', 'footers')).data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18846"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18812"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove all empty strings\n",
    "data = [text for text in data if text.strip() != \"\"]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = embedding_model.encode(newsgroup.data[:100], show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import voyageai\n",
    "\n",
    "# vo = voyageai.Client()\n",
    "\n",
    "# batch_size = 128\n",
    "# embeddings = []\n",
    "\n",
    "# # Use tqdm to show a progress bar\n",
    "# for i in tqdm(range(0, len(data), batch_size), desc=\"Embedding Batches\"):\n",
    "#     # Embed the batch of data\n",
    "#     batch_embeddings = vo.embed(\n",
    "#         data[i : i + batch_size],\n",
    "#         model=\"voyage-3\",\n",
    "#         input_type=\"document\",\n",
    "#     ).embeddings\n",
    "    \n",
    "#     # Append the embeddings to the list\n",
    "#     embeddings.append(batch_embeddings)\n",
    "\n",
    "# # Concatenate the embeddings into a single array\n",
    "# embeddings = np.concatenate(embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"embeddings.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(\"embeddings.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18812, 1024)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:10\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 18:25:42,598 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "2024-10-05 18:25:55,171 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-10-05 18:25:55,172 - BERTopic - Cluster - Start clustering the reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "!python -m spacy download en_core_web_lg\n",
    "representation_model = PartOfSpeech(model=\"en_core_web_lg\")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=5,\n",
    "    min_dist=0.0,\n",
    ")\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=15,\n",
    "    metric=\"euclidean\",\n",
    "    cluster_selection_method=\"eom\",\n",
    "    prediction_data=True,\n",
    ")\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer()\n",
    "\n",
    "model = BERTopic(\n",
    "    verbose=True,\n",
    "    min_topic_size=2,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    calculate_probabilities=True,\n",
    "    umap_model=umap_model,\n",
    "    # embedding_model=embedding_model,\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    representation_model=representation_model,\n",
    ")\n",
    "\n",
    "topics, probs = model.fit_transform(data, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''Based on the below information, extract and synthesize human-readable tags/keywords/themes from the text, capitalized first letters of words. What is the main human-readable theme or subject matter discussed in the provided texts? What is the overarching, high-level theme of the texts, e.g. \"Music\", \"Sports\", \"Environment\", etc.? Please provide overarching themes that tie the different pieces of information together. What is/are the overarching, highest level theme(s) that you could use as a keyword(s)? Prefer single word tags/keywords, e.g. \"Tennis\" rather than \"Tennis Match\", \"Prison\" rather than \"Prison Time\", etc.! Some examples of human-readable themes are   \"Agriculture\", \"Astronomy\", \"Chemistry\", \"Computational Universe\", \"Computer Systems\", \"Climate and Environment\", \"Culture\", \"Demographics\", \"Earth Science\", \"Economics\", \"Education\", \"Engineering\", \"Finance\", \"Geography\", \"Government\", \"Games\", \"Health\", \"History\", \"Human Activities\", \"Images\", \"Language\", \"Law\", \"Life Science\", \"Machine Learning\", \"Manufacturing\", \"Mathematics\", \"Medicine\", \"Meteorology\", \"Physical Sciences\", \"Politics\", \"Social Media\", \"Sociology\", \"Statistics\", \"Text & Literature\",  \"Transportation\". Also, don't give very similar tags/keywords, e.g. \"Wine\" and \"Red Wine\", just give one or the other in these cases. Avoid tags/keywords that are too specific, e.g. \"Serine Threonine Protein Kinase\". Good theme examples are: \"Birds\", \"Species Migration\", \"Air Pollution\", or \"War\", \"Government\", \"International Relations\", \"Politics\". Another important rule to obey - place more focus on the dataset names for theme extraction. And be concise in theme generation, e.g. instead of \"Income Prediction\", say \"Income\", instead of \"Demographic Information\", say \"Demographics\"! Also, extract the theme of the text, what it is about, instead of the type of problem it is, for instance we don't care about \"Regression\", \"Numerical Features\", \"Data Analysis\", \"Data\", \"Outliers\", \"Subsampling\" or things of that sort, but we care about the ESSENCE of the text! Say {\"Themes\": [...], \"Overarching themes\": [...]} and give your answer in JSON format.\n",
    "For example, for this text:\n",
    "Text 1: The Biden administration is preparing to roll out a sweeping border executive action as early as Tuesday, according to two sources familiar with the discussions, who cautioned that timing is fluid.\n",
    "\n",
    "White House officials have begun reaching out to mayors who represent cities along the US southern border to potentially join President Joe Biden when he announces the order, two other sources familiar with those conversations said.\n",
    "\n",
    "For weeks, administration officials have been working through an executive action that would dramatically limit migrants’ ability to seek asylum at the US southern border — part of a strategy to try to give Biden the upper hand on one of his Republican rival’s key campaign issues. The action is designed to potentially blunt Republican attacks on border security and preempt former President Donald Trump ahead of the first presidential debate, which will be held on June 27 on CNN.\n",
    "---\n",
    "Text 2: Now that a New York jury has convicted former President Donald Trump of all 34 felony charges of falsifying business records, the next obvious question is: Can a convicted felon run for president?\n",
    "\n",
    "Definitely.\n",
    "\n",
    "Trump meets all three requirements. There is, arguably, another criterion laid out in the 14th Amendment, where it states that no one who has previously taken an oath of office who engages in insurrection can be an officer of the US. But the US Supreme Court ruled earlier this year that Congress would have to pass a special law invoking this prohibition. That’s not happening any time soon.\n",
    "\n",
    "Judge Juan Merchan has scheduled Trump’s sentencing for July 11, which happens to be four days before the start of the Republican National Convention that is scheduled to take place in Milwaukee.\n",
    "a\n",
    "It is technically possible, although perhaps unlikely for a first-time offender, that Trump could be sentenced to prison time.\n",
    "---\n",
    "This would be your answer:\n",
    "{\"Themes\": [\"Biden Administration\", \"Border\", \"Executive Action\", \"Asylum\", \"Immigration\", \"Trump\", \"Felony\", \"Business Records\", \"Presidential Campaign\", \"Republican\", \"Debate\", \"Former President\", \"Conviction\", \"Sentencing\", \"Prison\", \"14th Amendment\", \"Insurrection\", \"Supreme Court\", \"Republican National Convention\"], \"Overarching themes\": [\"Politics\", \"Government\", \"Law\", \"Justice\", \"Elections\"]}\n",
    "---\n",
    "Now, the above was just an example. Now, do it for the following text(s), be concise!:\n",
    "[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "---\n",
    "Remember, extract and synthesize human-readable tags/keywords/themes from the text, capitalized first letters of words. What is the main human-readable theme or subject matter discussed in the provided texts? What is the overarching, high-level theme of the texts, e.g. \"Music\", \"Sports\", \"Environment\", etc.? Please provide overarching themes that tie the different pieces of information together. What is/are the overarching, highest level theme(s) that you could use as a keyword(s)? Prefer single word tags/keywords, e.g. \"Tennis\" rather than \"Tennis Match\", \"Prison\" rather than \"Prison Time\", etc.! Some examples of human-readable themes are   \"Agriculture\", \"Astronomy\", \"Chemistry\", \"Computational Universe\", \"Computer Systems\", \"Climate and Environment\", \"Culture\", \"Demographics\", \"Earth Science\", \"Economics\", \"Education\", \"Engineering\", \"Finance\", \"Geography\", \"Government\", \"Games\", \"Health\", \"History\", \"Human Activities\", \"Images\", \"Language\", \"Law\", \"Life Science\", \"Machine Learning\", \"Manufacturing\", \"Mathematics\", \"Medicine\", \"Meteorology\", \"Physical Sciences\", \"Politics\", \"Social Media\", \"Sociology\", \"Statistics\", \"Text & Literature\",  \"Transportation\". Also, don't give very similar tags/keywords, e.g. \"Wine\" and \"Red Wine\", just give one or the other in these cases. Avoid tags/keywords that are too specific, e.g. \"Serine Threonine Protein Kinase\". Good theme examples are: \"Birds\", \"Species Migration\", \"Air Pollution\", or \"War\", \"Government\", \"International Relations\", \"Politics\". Another important rule to obey - place more focus on the dataset names for theme extraction. And be concise in theme generation, e.g. instead of \"Income Prediction\", say \"Income\", instead of \"Demographic Information\", say \"Demographics\"! Also, extract the theme of the text, what it is about, instead of the type of problem it is, for instance we don't care about \"Regression\", \"Numerical Features\", \"Data Analysis\", \"Data\", \"Outliers\", \"Subsampling\" or things of that sort, but we care about the ESSENCE of the text! Say {\"Themes\": [...], \"Overarching themes\": [...]} and give your answer in JSON format.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 0.93582286, 0.68280542, 0.53965799],\n",
       "       [1.        , 1.        , 0.93582286, 0.68280542, 0.53965799],\n",
       "       [0.93582286, 0.93582286, 1.        , 0.69075243, 0.5471694 ],\n",
       "       [0.68280542, 0.68280542, 0.69075243, 1.        , 0.67244471],\n",
       "       [0.53965799, 0.53965799, 0.5471694 , 0.67244471, 1.        ]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # import cosine similarity\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # calculate cosine similarity between all sentences\n",
    "# cosine_sim = cosine_similarity(embeddings, embeddings)\n",
    "# cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# API_URL = \"https://se1nsjdwu8nlsqwt.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "# headers = {\n",
    "# \t\"Accept\" : \"application/json\",\n",
    "# \t\"Authorization\": \"Bearer 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\",\n",
    "# \t\"Content-Type\": \"application/json\" \n",
    "# }\n",
    "\n",
    "# def query(payload):\n",
    "# \tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "# \treturn response.json()\n",
    "\n",
    "# output = query({\n",
    "# \t\"inputs\": newsgroup.data[:31],\n",
    "# \t\"parameters\": {}\n",
    "# })\n",
    "\n",
    "# # same but for cycle for each individual sentence\n",
    "# # outputs = []\n",
    "# # for sentence in newsgroup.data[:32]:\n",
    "# #     outputs.append(query({\n",
    "# #         \"inputs\": sentence,\n",
    "# #         \"parameters\": {}\n",
    "# #     }))\n",
    "# # same but in batches of 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TagGPT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
